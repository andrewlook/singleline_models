{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sketch_rnn.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from singleline_models.dataset import StrokesDataset, create_dataloaders\n",
    "from singleline_models.sketch_rnn.sampler import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from singleline_models.utils import CN\n",
    "from singleline_models.lstm.all import LSTM_BUILTIN, LSTM_RNNLIB\n",
    "from singleline_models.sketch_rnn.layers import BivariateGaussianMixture, DecoderRNN, EncoderRNN, KLDivLoss, ReconstructionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SketchRNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hp, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.hp = hp\n",
    "        self.device = device\n",
    "        self.encoder = EncoderRNN(\n",
    "            self.hp.d_z,\n",
    "            self.hp.enc_hidden_size,\n",
    "            use_recurrent_dropout=self.hp.use_recurrent_dropout,\n",
    "            r_dropout_prob=self.hp.r_dropout_prob,\n",
    "            use_layer_norm=self.hp.use_layer_norm,\n",
    "            layer_norm_learnable=self.hp.layer_norm_learnable,\n",
    "            lstm_impl=self.hp.lstm_impl,\n",
    "        ).to(self.device)\n",
    "        self.decoder = DecoderRNN(\n",
    "            self.hp.d_z,\n",
    "            self.hp.dec_hidden_size,\n",
    "            self.hp.n_distributions,\n",
    "            use_recurrent_dropout=self.hp.use_recurrent_dropout,\n",
    "            r_dropout_prob=self.hp.r_dropout_prob,\n",
    "            use_layer_norm=self.hp.use_layer_norm,\n",
    "            layer_norm_learnable=self.hp.layer_norm_learnable,\n",
    "            lstm_impl=self.hp.lstm_impl,\n",
    "        ).to(self.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        C.architecture = 'Pytorch-SketchRNN'\n",
    "\n",
    "        C.dataset_source: str = 'look'\n",
    "        C.dataset_name: str = 'look_i16__minn10_epsilon1'\n",
    "        C.dataset_fname: str = 'data/look/look_i16__minn10_epsilon1.npz'\n",
    "\n",
    "        # C.dataset_source = 'look'\n",
    "        # C.dataset_name = 'epoch20240221_expanded10x_trainval'\n",
    "        # C.dataset_fname = 'data/look/epoch20240221_expanded10x_trainval.npz'\n",
    "        \n",
    "        # data augmentation\n",
    "        C.augment_stroke_prob = 0.1\n",
    "        C.use_random_scale = True\n",
    "        C.random_scale_factor = 0.15\n",
    "\n",
    "        # duration of training run\n",
    "        C.epochs = 50000\n",
    "        # how often to compute validation metrics / persist / sample\n",
    "        C.save_every_n_epochs = 100\n",
    "        # validate_every_n_epochs = 2\n",
    "\n",
    "        # adaptive learning rate\n",
    "        C.lr = 1e-3\n",
    "        C.use_lr_decay = False\n",
    "        C.min_lr = 1e-5\n",
    "        C.lr_decay = 0.9999\n",
    "\n",
    "        # recurrent dropout\n",
    "        C.use_recurrent_dropout = False\n",
    "        C.r_dropout_prob = 0.1\n",
    "\n",
    "        # layer normalization\n",
    "        C.use_layer_norm = True\n",
    "        C.layer_norm_learnable = False\n",
    "\n",
    "        # lstm_impl = LSTM_BUILTIN\n",
    "        C.lstm_impl = LSTM_RNNLIB\n",
    "        \n",
    "        # Encoder and decoder sizes\n",
    "        C.enc_hidden_size = 256\n",
    "        C.dec_hidden_size = 512\n",
    "\n",
    "        # Batch size\n",
    "        C.batch_size = 100\n",
    "\n",
    "        # Number of features in $z$\n",
    "        C.d_z = 128\n",
    "        # Number of distributions in the mixture, $M$\n",
    "        C.n_distributions = 20\n",
    "\n",
    "        # Weight of KL divergence loss, $w_{KL}$\n",
    "        C.kl_div_loss_weight = 0.5\n",
    "        # decaying weight of KL loss\n",
    "        C.use_eta = False\n",
    "        C.eta_min = 1e-2\n",
    "        C.eta_R = 0.99995\n",
    "\n",
    "        # Gradient clipping\n",
    "        C.grad_clip = 1.\n",
    "        # Temperature $\\tau$ for sampling\n",
    "        C.temperature = 0.4\n",
    "\n",
    "        # Filter out stroke sequences longer than $200$\n",
    "        C.max_seq_length = 200\n",
    "        return C\n",
    "\n",
    "    def sample(self, data: torch.Tensor, temperature: float):\n",
    "        # $N_{max}$\n",
    "        longest_seq_len = len(data)\n",
    "\n",
    "        # Get $z$ from the encoder\n",
    "        z, _, _ = self.encoder(data)\n",
    "\n",
    "        # Start-of-sequence stroke is $(0, 0, 1, 0, 0)$\n",
    "        s = data.new_tensor([0, 0, 1, 0, 0])\n",
    "        seq = [s]\n",
    "        # Initial decoder is `None`.\n",
    "        # The decoder will initialize it to $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
    "        state = None\n",
    "\n",
    "        # We don't need gradients\n",
    "        with torch.no_grad():\n",
    "            # Sample $N_{max}$ strokes\n",
    "            for i in range(longest_seq_len):\n",
    "                # $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ is the input to the decoder\n",
    "                data = torch.cat([s.view(1, 1, -1), z.unsqueeze(0)], 2)\n",
    "                # Get $\\Pi$, $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$,\n",
    "                # $q$ and the next state from the decoder\n",
    "                dist, q_logits, state = self.decoder(data, z, state)\n",
    "                # Sample a stroke\n",
    "                s = self._sample_step(dist, q_logits, temperature)\n",
    "                # Add the new stroke to the sequence of strokes\n",
    "                seq.append(s)\n",
    "                # Stop sampling if $p_3 = 1$. This indicates that sketching has stopped\n",
    "                if s[4] == 1:\n",
    "                    break\n",
    "\n",
    "        # Create a PyTorch tensor of the sequence of strokes\n",
    "        seq = torch.stack(seq)\n",
    "        return seq\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_step(dist: BivariateGaussianMixture, q_logits: torch.Tensor, temperature: float):\n",
    "        # Set temperature $\\tau$ for sampling. This is implemented in class `BivariateGaussianMixture`.\n",
    "        dist.set_temperature(temperature)\n",
    "        # Get temperature adjusted $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
    "        pi, mix = dist.get_distribution()\n",
    "        # Sample from $\\Pi$ the index of the distribution to use from the mixture\n",
    "        idx = pi.sample()[0, 0]\n",
    "\n",
    "        # Create categorical distribution $q$ with log-probabilities `q_logits` or $\\hat{q}$\n",
    "        q = torch.distributions.Categorical(logits=q_logits / temperature)\n",
    "        # Sample from $q$\n",
    "        q_idx = q.sample()[0, 0]\n",
    "\n",
    "        # Sample from the normal distributions in the mixture and pick the one indexed by `idx`\n",
    "        xy = mix.sample()[0, 0, idx]\n",
    "\n",
    "        # Create an empty stroke $(\\Delta x, \\Delta y, q_1, q_2, q_3)$\n",
    "        stroke = q_logits.new_zeros(5)\n",
    "        # Set $\\Delta x, \\Delta y$\n",
    "        stroke[:2] = xy\n",
    "        # Set $q_1, q_2, q_3$\n",
    "        stroke[q_idx + 2] = 1\n",
    "        #\n",
    "        return stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Trainer():\n",
    "    # Device configurations to pick the device to run the experiment\n",
    "    device: str\n",
    "    \n",
    "    model: SketchRNNModel\n",
    "    encoder: EncoderRNN\n",
    "    decoder: DecoderRNN\n",
    "    optimizer: optim.Adam\n",
    "    sampler: Sampler\n",
    "\n",
    "    train_loader: DataLoader\n",
    "    valid_loader: DataLoader\n",
    "    train_dataset: StrokesDataset\n",
    "    valid_dataset: StrokesDataset\n",
    "\n",
    "    kl_div_loss = KLDivLoss()\n",
    "    reconstruction_loss = ReconstructionLoss()\n",
    "\n",
    "    learning_rate: float\n",
    "    best_val_loss: float = float('inf')\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: SketchRNNModel,\n",
    "                 hp: CN,\n",
    "                 device=\"cuda\",\n",
    "                 models_dir=\"models\",\n",
    "                 use_wandb=False,\n",
    "                 wandb_project='sketchrnn-pytorch',\n",
    "                 wandb_entity='andrewlook'):\n",
    "        self.model = model\n",
    "\n",
    "        # TODO: remove these once I finish refactor\n",
    "        self.encoder = self.model.encoder\n",
    "        self.decoder = self.model.decoder\n",
    "\n",
    "        self.hp = hp\n",
    "        self.device = device\n",
    "        self.use_wandb = use_wandb\n",
    "        \n",
    "        # create a unique run ID, to distinguish saved model checkpoints / sample images\n",
    "        self.run_id = f\"{math.floor(np.random.rand() * 1e6):07d}\"\n",
    "        if self.use_wandb:\n",
    "            run = wandb.init(\n",
    "                project=wandb_project,\n",
    "                entity=wandb_entity,\n",
    "                config=hp.__dict__,\n",
    "            )\n",
    "            # use wandb's run ID, if available, so checkpoints match W&B's dashboard ID\n",
    "            self.run_id = run.id\n",
    "\n",
    "        print('='*60)\n",
    "        print(f\"RUN_ID: {self.run_id}\\n\")\n",
    "        print(f\"HYPERPARAMETERS:\\n\")\n",
    "        print(json.dumps(hp.__dict__, indent=2))\n",
    "        print('='*60 + '\\n\\n')\n",
    "\n",
    "        self.models_dir = Path(models_dir)\n",
    "        self.run_dir = self.models_dir / self.run_id\n",
    "        if not os.path.isdir(self.run_dir):\n",
    "            os.makedirs(self.run_dir)\n",
    "\n",
    "        # Initialize step count, to be updated in the training loop\n",
    "        self.total_steps = 0\n",
    "\n",
    "        if self.use_wandb:\n",
    "            wandb.watch((self.encoder, self.decoder), log=\"all\", log_freq=10, log_graph=True)\n",
    "\n",
    "        # store learning rate as state, so it can be modified by LR decay\n",
    "        self.learning_rate = self.hp.lr\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), self.learning_rate)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), self.learning_rate)\n",
    "\n",
    "        self.eta_step = self.hp.eta_min if self.hp.use_eta else 1\n",
    "\n",
    "        self.train_dataset, self.train_loader, self.valid_dataset, self.valid_loader = create_dataloaders(hp)\n",
    "        \n",
    "        # Pick 5 indices from the validation dataset, so the sampling can be compared across epochs\n",
    "        self.valid_idxs = [np.random.choice(len(self.valid_dataset)) for _ in range(5)]\n",
    "\n",
    "    def save(self, epoch):\n",
    "        torch.save(self.encoder.state_dict(), \\\n",
    "            Path(self.run_dir) / f'runid-{self.run_id}_epoch-{epoch:05d}_encoderRNN.pth')\n",
    "        torch.save(self.decoder.state_dict(), \\\n",
    "            Path(self.run_dir) / f'runid-{self.run_id}_epoch-{epoch:05d}_decoderRNN.pth')\n",
    "\n",
    "    def load(self, epoch):\n",
    "        extra_args = {}\n",
    "        if self.device != 'cuda':\n",
    "            extra_args=dict(map_location=torch.device('cpu'))\n",
    "        saved_encoder = torch.load(Path(self.run_dir) / f'runid-{self.run_id}_epoch-{epoch:05d}_encoderRNN.pth', **extra_args)\n",
    "        saved_decoder = torch.load(Path(self.run_dir) / f'runid-{self.run_id}_epoch-{epoch:05d}_decoderRNN.pth', **extra_args)\n",
    "        self.encoder.load_state_dict(saved_encoder)\n",
    "        self.decoder.load_state_dict(saved_decoder)\n",
    "    \n",
    "    def log(self, metrics):\n",
    "        if self.use_wandb:\n",
    "            wandb.log(metrics, step=self.total_steps)\n",
    "        else:\n",
    "            pass\n",
    "            #pprint({'step': self.total_steps, **metrics})\n",
    "\n",
    "    def sample(self, epoch, display=False):\n",
    "        orig_paths = []\n",
    "        decoded_paths = []\n",
    "        for idx in self.valid_idxs:\n",
    "            orig_path = self.run_dir / f'runid-{self.run_id}_epoch-{epoch:05d}_sample-{idx:04d}_orig.png'\n",
    "            decoded_path = self.run_dir / f'runid-{self.run_id}_epoch-{epoch:05d}_sample-{idx:04d}_decoded.png'\n",
    "\n",
    "            # Randomly pick a sample from validation dataset to encoder\n",
    "            data, *_ = self.valid_dataset[idx]\n",
    "            Sampler.plot(data, orig_path)\n",
    "\n",
    "            # Add batch dimension and move it to device\n",
    "            data_batched = data.unsqueeze(1).to(self.device)\n",
    "            # Sample\n",
    "            sampled_seq = self.model.sample(data_batched, self.hp.temperature)\n",
    "            Sampler.plot(sampled_seq, decoded_path)\n",
    "\n",
    "            if display:\n",
    "                Image.open(orig_path).show()\n",
    "                Image.open(decoded_path).show()\n",
    "            orig_paths.append(orig_path)\n",
    "            decoded_paths.append(decoded_path)\n",
    "        return sorted(orig_paths), sorted(decoded_paths)   \n",
    "\n",
    "    def step(self, batch: Any, is_training=False):\n",
    "        self.encoder.train(is_training)\n",
    "        self.decoder.train(is_training)\n",
    "\n",
    "        # Move `data` and `mask` to device and swap the sequence and batch dimensions.\n",
    "        # `data` will have shape `[seq_len, batch_size, 5]` and\n",
    "        # `mask` will have shape `[seq_len, batch_size]`.\n",
    "        data = batch[0].to(self.device).transpose(0, 1)\n",
    "        mask = batch[1].to(self.device).transpose(0, 1)\n",
    "        batch_items = len(data)\n",
    "\n",
    "        # print(f\"Trainer.step - data: {data.shape}\")\n",
    "        # print(data[:5,0])\n",
    "        \n",
    "        # Get $z$, $\\mu$, and $\\hat{\\sigma}$\n",
    "        z, mu, sigma_hat = self.encoder(data)\n",
    "\n",
    "        # Concatenate $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$\n",
    "        z_stack = z.unsqueeze(0).expand(data.shape[0] - 1, -1, -1)\n",
    "        inputs = torch.cat([data[:-1], z_stack], 2)\n",
    "        # Get mixture of distributions and $\\hat{q}$\n",
    "        dist, q_logits, _ = self.decoder(inputs, z, None)\n",
    "\n",
    "        # $L_{KL}$\n",
    "        kl_loss = self.kl_div_loss(sigma_hat, mu)\n",
    "        if self.hp.use_eta:\n",
    "            kl_loss *= self.eta_step\n",
    "\n",
    "        # $L_R$\n",
    "        reconstruction_loss = self.reconstruction_loss(mask, data[1:], dist, q_logits)\n",
    "        # $Loss = L_R + w_{KL} L_{KL}$\n",
    "        loss = reconstruction_loss + self.hp.kl_div_loss_weight * kl_loss\n",
    "\n",
    "        # Only if we are in training state\n",
    "        if is_training:\n",
    "            # Set `grad` to zero\n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "            # Clip gradients\n",
    "            nn.utils.clip_grad_norm_(self.encoder.parameters(), self.hp.grad_clip)\n",
    "            nn.utils.clip_grad_norm_(self.decoder.parameters(), self.hp.grad_clip)\n",
    "            # Optimize\n",
    "            self.encoder_optimizer.step()\n",
    "            self.decoder_optimizer.step()\n",
    "        return loss.item(), reconstruction_loss.item(), kl_loss.item(), batch_items\n",
    "\n",
    "    def validate_one_epoch(self, epoch):\n",
    "        total_items, total_loss, total_kl_loss, total_reconstruction_loss = 0, 0, 0, 0\n",
    "        with torch.no_grad():    \n",
    "            for batch in iter(self.valid_loader):\n",
    "                loss, reconstruction_loss, kl_loss, batch_items = self.step(batch, is_training=False)\n",
    "\n",
    "                total_loss += loss * batch_items\n",
    "                total_reconstruction_loss += reconstruction_loss * batch_items\n",
    "                total_kl_loss += kl_loss * batch_items\n",
    "                total_items += batch_items\n",
    "                \n",
    "        avg_loss = total_loss / total_items\n",
    "        avg_reconstruction_loss = total_reconstruction_loss / total_items\n",
    "        avg_kl_loss = total_kl_loss / total_items\n",
    "        self.log(dict(\n",
    "            val_avg_loss=avg_loss,\n",
    "            val_avg_reconstruction_loss=avg_reconstruction_loss,\n",
    "            val_avg_kl_loss=avg_kl_loss,\n",
    "            epoch=epoch))\n",
    "        return avg_loss, avg_reconstruction_loss, avg_kl_loss\n",
    "\n",
    "    def train_one_epoch(self, epoch, parent_progressbar=None):\n",
    "        steps_per_epoch = len(self.train_loader)\n",
    "        for idx, batch in enumerate(progress_bar(iter(self.train_loader), parent=parent_progressbar)):\n",
    "            self.total_steps = idx + epoch * steps_per_epoch\n",
    "            loss, reconstruction_loss, kl_loss, _ = self.step(batch, is_training=True)\n",
    "            self.log(dict(\n",
    "                loss=loss,\n",
    "                reconstruction_loss=reconstruction_loss,\n",
    "                kl_loss=kl_loss,\n",
    "                epoch=epoch,\n",
    "                learning_rate=self.learning_rate,\n",
    "                eta_step=self.eta_step))\n",
    "        # update learning rate, if use_lr_decay is enabled\n",
    "        if self.hp.use_lr_decay:\n",
    "            if self.learning_rate > self.hp.min_lr:\n",
    "                self.learning_rate *= self.hp.lr_decay\n",
    "            self.encoder_optimizer = self.update_lr(self.encoder_optimizer, self.learning_rate)\n",
    "            self.decoder_optimizer = self.update_lr(self.decoder_optimizer, self.learning_rate)\n",
    "        # update weight of KL loss, if use_eta is enabled\n",
    "        if self.hp.use_eta:\n",
    "            self.eta_step = 1-(1-self.hp.eta_min)*self.hp.eta_R\n",
    "\n",
    "    def update_lr(self, optimizer, lr):\n",
    "        \"\"\"Decay learning rate by a factor of lr_decay\"\"\"\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return optimizer\n",
    "        \n",
    "    def train(self):\n",
    "        mb = master_bar(range(self.hp.epochs))\n",
    "        for epoch in mb:\n",
    "            self.train_one_epoch(epoch=epoch, parent_progressbar=mb)\n",
    "            val_avg_loss, *_ = self.validate_one_epoch(epoch)\n",
    "            update_best_val = False\n",
    "            if val_avg_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_avg_loss\n",
    "                update_best_val = True\n",
    "                #if epoch % self.hp.save_every_n_epochs == 0:\n",
    "                self.save(epoch=0)\n",
    "                self.sample(epoch=0)\n",
    "            mb.write(f\"Finished epoch {epoch}. Validation Loss: {val_avg_loss}{' (new best)' if update_best_val else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
