{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from singleline_models.utils import CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StrokesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ## Dataset\n",
    "\n",
    "    This class loads and pre-processes the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: np.array, max_seq_length: int, scale: Optional[float] = None):\n",
    "        \"\"\"\n",
    "        `dataset` is a list of numpy arrays of shape [seq_len, 3].\n",
    "        It is a sequence of strokes, and each stroke is represented by\n",
    "        3 integers.\n",
    "        First two are the displacements along x and y ($\\Delta x$, $\\Delta y$)\n",
    "        and the last integer represents the state of the pen, $1$ if it's touching\n",
    "        the paper and $0$ otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        data = []\n",
    "        # We iterate through each of the sequences and filter\n",
    "        for idx, seq in enumerate(dataset):\n",
    "            if len(seq) < 10:\n",
    "                print(f\"filtering out {idx} - length: {len(seq)}\")\n",
    "                continue\n",
    "            elif len(seq) > max_seq_length:\n",
    "                print(f\"truncating {idx} - length: {len(seq)}\")\n",
    "                seq = seq[:max_seq_length]\n",
    "            # Clamp $\\Delta x$, $\\Delta y$ to $[-1000, 1000]$\n",
    "            seq = np.minimum(seq, 1000)\n",
    "            seq = np.maximum(seq, -1000)\n",
    "            # Convert to a floating point array and add to `data`\n",
    "            seq = np.array(seq, dtype=np.float32)\n",
    "            data.append(seq)\n",
    "        print(f\"finished filtering - len(dataset) = {len(dataset)}, len(data) = {len(data)}\")\n",
    "\n",
    "        # We then calculate the scaling factor which is the\n",
    "        # standard deviation of ($\\Delta x$, $\\Delta y$) combined.\n",
    "        # Paper notes that the mean is not adjusted for simplicity,\n",
    "        # since the mean is anyway close to $0$.\n",
    "        if scale is None:\n",
    "            scale = np.std(np.concatenate([np.ravel(s[:, 0:2]) for s in data]))\n",
    "        self.scale = scale\n",
    "\n",
    "        # Get the longest sequence length among all sequences\n",
    "        longest_seq_len = max([len(seq) for seq in data])\n",
    "\n",
    "        # We initialize PyTorch data array with two extra steps for start-of-sequence (sos)\n",
    "        # and end-of-sequence (eos).\n",
    "        # Each step is a vector $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
    "        # Only one of $p_1, p_2, p_3$ is $1$ and the others are $0$.\n",
    "        # They represent *pen down*, *pen up* and *end-of-sequence* in that order.\n",
    "        # $p_1$ is $1$ if the pen touches the paper in the next step.\n",
    "        # $p_2$ is $1$ if the pen doesn't touch the paper in the next step.\n",
    "        # $p_3$ is $1$ if it is the end of the drawing.\n",
    "        self.data = torch.zeros(len(data), longest_seq_len + 2, 5, dtype=torch.float)\n",
    "        # The mask array needs only one extra-step since it is for the outputs of the\n",
    "        # decoder, which takes in `data[:-1]` and predicts next step.\n",
    "        self.mask = torch.zeros(len(data), longest_seq_len + 1)\n",
    "\n",
    "        for i, seq in enumerate(data):\n",
    "            seq = torch.from_numpy(seq)\n",
    "            len_seq = len(seq)\n",
    "            # Scale and set $\\Delta x, \\Delta y$\n",
    "            self.data[i, 1:len_seq + 1, :2] = seq[:, :2] / scale\n",
    "            # $p_1$\n",
    "            self.data[i, 1:len_seq + 1, 2] = 1 - seq[:, 2]\n",
    "            # $p_2$\n",
    "            self.data[i, 1:len_seq + 1, 3] = seq[:, 2]\n",
    "            # $p_3$\n",
    "            self.data[i, len_seq + 1:, 4] = 1\n",
    "            # Mask is on until end of sequence\n",
    "            self.mask[i, :len_seq + 1] = 1\n",
    "\n",
    "        # Start-of-sequence is $(0, 0, 1, 0, 0)$\n",
    "        self.data[:, 0, 2] = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Size of the dataset\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Get a sample\"\"\"\n",
    "        return self.data[idx], self.mask[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_scale(data, random_scale_factor=0.15):\n",
    "    \"\"\"Augment data by stretching x and y axis randomly [1-e, 1+e].\"\"\"\n",
    "    x_scale_factor = (\n",
    "        np.random.random() - 0.5) * 2 * random_scale_factor + 1.0\n",
    "    y_scale_factor = (\n",
    "        np.random.random() - 0.5) * 2 * random_scale_factor + 1.0\n",
    "    result = np.copy(data)\n",
    "    result[:, 0] *= x_scale_factor\n",
    "    result[:, 1] *= y_scale_factor\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def augment_strokes(strokes, prob=0.0):\n",
    "    \"\"\"Perform data augmentation by randomly dropping out strokes.\"\"\"\n",
    "    # drop each point within a line segments with a probability of prob\n",
    "    # note that the logic in the loop prevents points at the ends to be dropped.\n",
    "    result = []\n",
    "    prev_stroke = [0, 0, 1]\n",
    "    count = 0\n",
    "    stroke = [0, 0, 1]  # Added to be safe.\n",
    "    for i in range(len(strokes)):\n",
    "        candidate = [strokes[i][0], strokes[i][1], strokes[i][2]]\n",
    "        if candidate[2] == 1 or prev_stroke[2] == 1:\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "        urnd = np.random.rand()  # uniform random variable\n",
    "        if candidate[2] == 0 and prev_stroke[2] == 0 and count > 2 and urnd < prob:\n",
    "            stroke[0] += candidate[0]\n",
    "            stroke[1] += candidate[1]\n",
    "        else:\n",
    "            stroke = candidate\n",
    "            prev_stroke = stroke\n",
    "            result.append(stroke)\n",
    "    return np.array(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_dataloaders(hp: CN):\n",
    "    path = Path(hp.dataset_fname)\n",
    "    if not hp.dataset_fname:\n",
    "        # `npz` file path is `data/quickdraw/[DATASET NAME].npz`\n",
    "        base_path = Path(f\"data/{hp.dataset_source}\")\n",
    "        path = base_path / f'{hp.dataset_name}.npz'\n",
    "    # Load the numpy file\n",
    "    dataset = np.load(str(path), encoding='latin1', allow_pickle=True)\n",
    "\n",
    "    # Create training dataset\n",
    "    train_dataset = StrokesDataset(dataset['train'], hp.max_seq_length)\n",
    "    # Create validation dataset\n",
    "    valid_dataset = StrokesDataset(dataset['valid'], hp.max_seq_length, train_dataset.scale)\n",
    "\n",
    "    def collate_fn(batch, **kwargs):\n",
    "        assert type(batch) == list\n",
    "        # assert len(batch) == hp.batch_size\n",
    "\n",
    "        all_data = []\n",
    "        all_mask = []\n",
    "        for data, mask in batch:\n",
    "            # print(f\"data.shape[0]={data.shape[0]}\")\n",
    "            assert data.shape[0] == hp.max_seq_length + 2\n",
    "            assert data.shape[1] == 5\n",
    "            assert len(data.shape) == 2\n",
    "            assert mask.shape[0] == hp.max_seq_length + 1\n",
    "            assert len(mask.shape) == 1\n",
    "\n",
    "            _data = data\n",
    "            if hp.use_random_scale:\n",
    "                _data = random_scale(data, hp.random_scale_factor)\n",
    "\n",
    "            if hp.augment_stroke_prob > 0:\n",
    "                _data = augment_strokes(_data, hp.augment_stroke_prob)\n",
    "\n",
    "            all_data.append(data)\n",
    "            all_mask.append(mask)\n",
    "\n",
    "\n",
    "        # print(f\"collate - batch: {len(batch)}, {batch[0][0].shape}, {batch[0][1].shape}\")\n",
    "        # print(f\"collate - kwargs: {kwargs}\")\n",
    "        return torch.stack(all_data), torch.stack(all_mask)\n",
    "\n",
    "    # Create training data loader\n",
    "    train_loader = DataLoader(train_dataset, hp.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    # Create validation data loader\n",
    "    valid_loader = DataLoader(valid_dataset, hp.batch_size)\n",
    "\n",
    "    return train_dataset, train_loader, valid_dataset, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
