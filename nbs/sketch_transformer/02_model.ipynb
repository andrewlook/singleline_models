{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sketch_transformer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from singleline_models.sketch_transformer.masks import *\n",
    "from singleline_models.sketch_transformer.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, max_seq_len, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Linear(5, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout_rate, max_len=max_seq_len)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, max_seq_len=max_seq_len, dropout_prob=dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        seq_len = x.shape[1]\n",
    "        x = self.embedding(x) # (batch, seq, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model))\n",
    "        # add positional embedding, and apply dropout\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        for i, layer in enumerate(self.enc_layers):\n",
    "            x = layer(x, mask)\n",
    "        return x # (batch, seq, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff,\n",
    "                    max_seq_len=1000,\n",
    "                    dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Linear(5, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout_rate, max_len=max_seq_len)\n",
    "\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff, max_seq_len=max_seq_len, dropout_prob=dropout_rate)\n",
    "                            for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, enc_output, padding_mask, dec_target_padding_mask, look_ahead_mask):\n",
    "        seq_len = x.shape[1]\n",
    "        x = self.embedding(x) # (batch, seq, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model))\n",
    "        # add positional embedding, and apply dropout\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        attention_weights = {}\n",
    "        for i, layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = layer(x, enc_output, padding_mask, dec_target_padding_mask, look_ahead_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReconstructionLoss(nn.Module):\n",
    "   \n",
    "    def forward(self, pred, real):\n",
    "        pred_locations = pred[:, :, :2]\n",
    "        real_locations = real[:, :, :2]\n",
    "        location_loss = F.mse_loss(pred_locations, real_locations, reduction='none') # [batch_size, n_seq, 2]\n",
    "        location_loss = torch.mean(location_loss, dim=2) # [batch_size, n_seq]\n",
    "\n",
    "        pred_metadata = pred[:, :, 2:] # un-normalized logits\n",
    "        real_metadata = real[:, :, 2:] # true labels in probability spaces (add up to 1).\n",
    "        # for K-dimensional inputs, torch cross_entropy() requires:\n",
    "        # - batch as first dimension,\n",
    "        # - class as second,\n",
    "        # - ... other dimensions\n",
    "        pred_metadata = pred_metadata.transpose(2, 1) # [batch_size, n_classes, n_seq]\n",
    "        real_idx = torch.argmax(real_metadata.transpose(2, 1), dim=1) # [batch_size, n_seq]\n",
    "        metadata_loss = F.cross_entropy(pred_metadata, real_idx, reduction='none') # [batch_size, n_seq]\n",
    "\n",
    "        # final dimension is \"end-of-sequence\" - invert it to get a mask for valid parts of the sequence\n",
    "        mask = torch.abs(real[..., -1]-1) # [batch_size, n_seq] \n",
    "\n",
    "        # slightly less efficient to mask & average these parts of the loss separately, but this allows\n",
    "        # logging them separately during training runs.\n",
    "        location_loss *= mask\n",
    "        location_loss = torch.mean(location_loss)\n",
    "        metadata_loss *= mask\n",
    "        metadata_loss = torch.mean(metadata_loss)\n",
    "\n",
    "        loss = location_loss + metadata_loss\n",
    "        \n",
    "        return loss, dict(location_loss=location_loss, metadata_loss=metadata_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super().__init__()\n",
    "        self.hp = hp\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=hp.n_layer,\n",
    "            d_model=hp.d_model,\n",
    "            num_heads=hp.n_head,\n",
    "            d_ff=hp.d_ff,\n",
    "            max_seq_len=hp.max_seq_length+2,\n",
    "            dropout_rate=hp.dropout_rate)\n",
    "        self.bottleneck_layer = SelfAttn(d_model=hp.d_model, d_lowerdim=hp.d_lowerdim)\n",
    "        self.expand_layer = DenseExpander(in_dim=hp.d_lowerdim, out_dim=hp.d_model, seq_len=hp.max_seq_length+2)\n",
    "        self.decoder = Decoder(num_layers=hp.n_layer,\n",
    "            d_model=hp.d_model,\n",
    "            num_heads=hp.n_head,\n",
    "            d_ff=hp.d_ff,\n",
    "            max_seq_len=hp.max_seq_length+2,\n",
    "            dropout_rate=hp.dropout_rate)\n",
    "        self.output_layer = nn.Linear(hp.d_model, 5)\n",
    "    \n",
    "    def encode(self, x, mask):\n",
    "        enc_output = self.encoder(x, mask)\n",
    "        lowerdim_output, _ = self.bottleneck_layer(enc_output)\n",
    "        return lowerdim_output, enc_output\n",
    "    \n",
    "    def decode(self, embedding, target, dec_padding_mask, dec_target_padding_mask, look_ahead_mask):\n",
    "        padding_mask = torch.zeros_like(dec_padding_mask).bool() if self.hp.blind_decoder_mask else dec_padding_mask\n",
    "        pre_decoder = self.expand_layer(embedding)\n",
    "        \n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            x=target,\n",
    "            enc_output=pre_decoder,\n",
    "            padding_mask=padding_mask,\n",
    "            dec_target_padding_mask=dec_target_padding_mask,\n",
    "            look_ahead_mask=look_ahead_mask)\n",
    "        final_output = self.output_layer(dec_output)\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def forward(self, input_seq, target_seq, enc_padding_mask, dec_padding_mask, dec_target_padding_mask, look_ahead_mask):\n",
    "        lowerdim_output, enc_output = self.encode(input_seq, enc_padding_mask)\n",
    "        \n",
    "        final_output, attention_weights = self.decode(\n",
    "            embedding=lowerdim_output,\n",
    "            target=target_seq,\n",
    "            dec_padding_mask=dec_padding_mask,\n",
    "            dec_target_padding_mask=dec_target_padding_mask,\n",
    "            look_ahead_mask=look_ahead_mask)\n",
    "        \n",
    "        return final_output, lowerdim_output #, enc_output, attention_weights\n",
    "\n",
    "    def encode_from_seq(self, inp_seq):\n",
    "        enc_padding_mask = create_padding_mask(inp_seq)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            res = self.encode(inp_seq, enc_padding_mask)\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
