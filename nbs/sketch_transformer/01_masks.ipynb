{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masks\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sketch_transformer.masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_padding_mask(seq):\n",
    "  \"\"\"\n",
    "  In seq, the 5th entry in the last dimension is the padding column, which will\n",
    "  be 1 if the row is padding.\n",
    "\n",
    "  In this case, we're just inverting that field to get a padding mask. Note:\n",
    "  this will not work for tokenizer-based sequences.\n",
    "\n",
    "  :param seq: (batch_size, seq_len, 5)\n",
    "  :return: (batch_size, seq_len)\n",
    "  \"\"\"\n",
    "  return torch.abs(seq[..., -1]-1)\n",
    "\n",
    "\n",
    "def create_lookahead_mask(seq_len):\n",
    "  return torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "\n",
    "\n",
    "def create_masks(input_seq, target_seq, device='cuda'):\n",
    "\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(input_seq)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(input_seq)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_lookahead_mask(target_seq.shape[1])\n",
    "    dec_target_padding_mask = create_padding_mask(target_seq)\n",
    "\n",
    "    # print(target_seq.shape, look_ahead_mask.shape, dec_target_padding_mask.shape)\n",
    "\n",
    "    # NOTE: torch nn.MHA takes separate padding & attn masks w/ different shapes,\n",
    "    #       so use that instead of combining here. TODO: check the source for how\n",
    "    #       they combine.\n",
    "    #\n",
    "    # # TODO: WTF is combined_mask used for???\n",
    "    # # TODO: can I verify this...?\n",
    "    # combined_mask = torch.fmax(look_ahead_mask, dec_target_padding_mask.unsqueeze(1))\n",
    "\n",
    "    return enc_padding_mask.to(device), dec_padding_mask.to(device), dec_target_padding_mask.to(device), look_ahead_mask.to(device)\n",
    "\n",
    "\n",
    "def make_dummy_input(total_seq_len, nattn, batch_size):\n",
    "  nignore = total_seq_len - nattn\n",
    "  return torch.cat([\n",
    "      torch.ones(batch_size, nattn, 5) * torch.tensor([0., 0., 0., 0., 0.]),\n",
    "      torch.ones(batch_size, nignore, 5) * torch.tensor([0., 0., 0., 0., 1.])\n",
    "  ], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
