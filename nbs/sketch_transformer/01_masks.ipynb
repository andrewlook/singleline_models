{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masks\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sketch_transformer.masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For attention masking, pytorch `nn.MultiHeadAttention` accepts either float or boolean masks.\n",
    "\n",
    "There was a bug with float masks, causing `Nan` values to get generated sometimes:\n",
    "- [regression - nn.MultiheadAttention does not respect adding of floating point mask to attention for the fast path · Issue #107084 · pytorch/pytorch](https://github.com/pytorch/pytorch/issues/107084)\n",
    "- [TransformerEncoderLayer fast path predicts NaN when provided attention bias · Issue #118628 · pytorch/pytorch](https://github.com/pytorch/pytorch/issues/118628)\n",
    "- [Disable nn.MHA fastpath for floating point masks by mikaylagawarecki · Pull Request #107641 · pytorch/pytorch](https://github.com/pytorch/pytorch/pull/107641)\n",
    "\n",
    "So I'm using boolean masks instead. Note: pytorch converts `1 == True`, `0 == False`.\n",
    "\n",
    "From the docs for [MultiheadAttention.forward()](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward):\n",
    "- `key_padding_mask` – If specified, a mask of shape (N,S) indicating which elements within key to ignore for the purpose of attention (i.e. treat as “padding”). For unbatched query, shape should be (S). Binary and float masks are supported. **For a binary mask, a `True` value indicates that the corresponding key value will be ignored for the purpose of attention.** For a float mask, it will be directly added to the corresponding key value.\n",
    "- `attn_mask` – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape \n",
    " (L,S) or (N⋅num_heads,L,S), where N is the batch size, L is the target sequence length, and S is the source sequence length. A 2D mask will be broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch. Binary and float masks are supported. **For a binary mask, a True value indicates that the corresponding position is not allowed to attend.** For a float mask, the mask values will be added to the attention weight. If both attn_mask and key_padding_mask are supplied, their types should match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    In seq, the 5th entry in the last dimension is the padding column, which will\n",
    "    be 1 if the row is padding.\n",
    "    \n",
    "    Convert to a boolean tensor, indicating 'True' for entries that are padding and should be ignored.\n",
    "\n",
    "    :param seq: (batch_size, seq_len, 5)\n",
    "    :return: (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    return seq[..., -1].bool()\n",
    "\n",
    "\n",
    "def create_lookahead_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create an attention mask, with rows representing target position and columns representing source position.\n",
    "\n",
    "    For row=i, column=j, mask[i][j] is 'True' if the decoder must ignore position j when processing position i.\n",
    "\n",
    "    An upper diagonal matrix (without the diagonal) will have 'True' for any j > i.\n",
    "    \n",
    "    :param seq_len: sequence length\n",
    "    :return: (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "\n",
    "def create_masks(input_seq, target_seq, device='cuda'):\n",
    "    enc_padding_mask = create_padding_mask(input_seq)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(input_seq)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_lookahead_mask(target_seq.shape[1])\n",
    "    dec_target_padding_mask = create_padding_mask(target_seq)\n",
    "\n",
    "    # NOTE: torch nn.MHA takes separate padding & attn masks w/ different shapes,\n",
    "    #       so use that instead of combining here\n",
    "    return enc_padding_mask.to(device), dec_padding_mask.to(device), dec_target_padding_mask.to(device), look_ahead_mask.to(device)\n",
    "\n",
    "\n",
    "def make_dummy_input(total_seq_len, nattn, batch_size):\n",
    "  nignore = total_seq_len - nattn\n",
    "  return torch.cat([\n",
    "      torch.ones(batch_size, nattn, 5) * torch.tensor([0., 0., 0., 0., 0.]),\n",
    "      torch.ones(batch_size, nignore, 5) * torch.tensor([0., 0., 0., 0., 1.])\n",
    "  ], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
