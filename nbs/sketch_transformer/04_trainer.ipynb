{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sketch_transformer.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from singleline_models.dataset import StrokesDataset, create_dataloaders\n",
    "from singleline_models.utils import CN\n",
    "\n",
    "from singleline_models.sketch_transformer.masks import create_masks\n",
    "from singleline_models.sketch_transformer.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_default_config():\n",
    "    C = CN()\n",
    "    C.n_layer = 4\n",
    "    C.n_head = 8\n",
    "    C.d_model =  128\n",
    "    C.d_ff = 512\n",
    "    C.d_lowerdim = 256\n",
    "\n",
    "    # these options must be filled in externally\n",
    "    C.vocab_size = None\n",
    "    C.block_size = None\n",
    "    # dropout hyperparameters\n",
    "    C.dropout_rate = 0.1\n",
    "\n",
    "\n",
    "    C.max_seq_length = 250\n",
    "    C.batch_size = 100\n",
    "\n",
    "    # if True, the decoder knows padding location of the input\n",
    "    C.blind_decoder_mask = True\n",
    "\n",
    "    # TODO: just make this a path?\n",
    "    # C.dataset_source: str = 'look'\n",
    "    # C.dataset_name: str = 'epoch20240221_expanded10x_trainval'\n",
    "    # C.dataset_fname: str = 'data/look/epoch20240221_expanded10x_trainval.npz'\n",
    "    C.dataset_source: str = 'look'\n",
    "    C.dataset_name: str = 'look_i16__minn10_epsilon1'\n",
    "    C.dataset_fname: str = 'data/look/look_i16__minn10_epsilon1.npz'\n",
    "    # data augmentation\n",
    "    C.augment_stroke_prob = 0.1\n",
    "    C.use_random_scale = True\n",
    "    C.random_scale_factor = 0.15\n",
    "\n",
    "    C.epochs = 50000\n",
    "    C.lr = 1e-3\n",
    "    C.use_lr_decay = True\n",
    "    C.min_lr = 1e-5\n",
    "    C.lr_decay = 0.9999\n",
    "    \n",
    "    return C\n",
    "\n",
    "hp = get_default_config()\n",
    "class Trainer():\n",
    "    # Device configurations to pick the device to run the experiment\n",
    "    device: str\n",
    "    \n",
    "    model: Model\n",
    "    loss: ReconstructionLoss\n",
    "    optimizer: optim.Adam\n",
    "    # sampler: Sampler\n",
    "\n",
    "    train_loader: DataLoader\n",
    "    valid_loader: DataLoader\n",
    "    train_dataset: StrokesDataset\n",
    "    valid_dataset: StrokesDataset\n",
    "\n",
    "    learning_rate: float\n",
    "    best_val_loss: float = float('inf')\n",
    "\n",
    "    def __init__(self,\n",
    "                 hp: CN,\n",
    "                 device=\"cuda\",\n",
    "                 models_dir=\"models\",\n",
    "                 use_wandb=False,\n",
    "                 wandb_project='sketch-transformer',\n",
    "                 wandb_entity='andrewlook'):\n",
    "        self.hp = hp\n",
    "        self.device = device\n",
    "        self.use_wandb = use_wandb\n",
    "        \n",
    "        # create a unique run ID, to distinguish saved model checkpoints / sample images\n",
    "        self.run_id = f\"{math.floor(np.random.rand() * 1e6):07d}\"\n",
    "        if self.use_wandb:\n",
    "            run = wandb.init(\n",
    "                project=wandb_project,\n",
    "                entity=wandb_entity,\n",
    "                config=hp.__dict__,\n",
    "            )\n",
    "            # use wandb's run ID, if available, so checkpoints match W&B's dashboard ID\n",
    "            self.run_id = run.id\n",
    "        self.models_dir = Path(models_dir)\n",
    "        self.run_dir = self.models_dir / self.run_id\n",
    "        if not os.path.isdir(self.run_dir):\n",
    "            os.makedirs(self.run_dir)\n",
    "        \n",
    "        print('='*60)\n",
    "        print(f\"RUN_ID: {self.run_id}\\n\")\n",
    "        print(f\"HYPERPARAMETERS:\\n\")\n",
    "        print(json.dumps(hp.__dict__, indent=2))\n",
    "        print('='*60 + '\\n\\n')\n",
    "\n",
    "        # Initialize step count, to be updated in the training loop\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.model = Model(hp=self.hp).to(self.device)\n",
    "        self.loss = ReconstructionLoss()\n",
    "\n",
    "        if self.use_wandb:\n",
    "            wandb.watch(self.model, log=\"all\", log_freq=10, log_graph=True)\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=3e-4)\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(optimizer=self.optimizer, num_warmup_steps=1000, num_training_steps=50000)\n",
    "\n",
    "        self.train_dataset, self.train_loader, self.valid_dataset, self.valid_loader = create_dataloaders(hp)\n",
    "\n",
    "        # # Create sampler\n",
    "        # self.sampler = Sampler(self.encoder, self.decoder)\n",
    "        # # Pick 5 indices from the validation dataset, so the sampling can be compared across epochs\n",
    "        # self.valid_idxs = [np.random.choice(len(self.valid_dataset)) for _ in range(5)]\n",
    "\n",
    "    def save(self):\n",
    "        model_path = Path(self.run_dir) / f'runid-{self.run_id}.pth'\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        with open(Path(self.run_dir) / f'runid-{self.run_id}.json', 'w') as outfile:\n",
    "            json.dump(self.hp.__dict__, outfile, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(self, **trainer_args):\n",
    "        with open(Path(self.run_dir) / f'runid-{self.run_id}.json', 'r') as infile:\n",
    "            saved_hp = json.load(infile)\n",
    "        hp = get_default_config()\n",
    "        hp.merge_from_dict(saved_hp)\n",
    "        trainer = Trainer(hp=hp, **trainer_args)\n",
    "        \n",
    "        extra_args = {}\n",
    "        if trainer.device != 'cuda':\n",
    "            extra_args=dict(map_location=torch.device('cpu'))\n",
    "        saved_model = torch.load(Path(self.run_dir) / f'runid-{self.run_id}.pth', **extra_args)\n",
    "\n",
    "        trainer.model.load_state_dict(saved_model)\n",
    "        return trainer\n",
    "    \n",
    "    def log(self, metrics):\n",
    "        if self.use_wandb:\n",
    "            wandb.log(metrics, step=self.total_steps)\n",
    "\n",
    "    # def sample(self, epoch, display=False):\n",
    "    #     orig_paths = []\n",
    "    #     decoded_paths = []\n",
    "    #     for idx in self.valid_idxs:\n",
    "    #         orig_path = self.run_dir / f'runid-{self.run_id}_epoch-{epoch:05d}_sample-{idx:04d}_orig.png'\n",
    "    #         decoded_path = self.run_dir / f'runid-{self.run_id}_epoch-{epoch:05d}_sample-{idx:04d}_decoded.png'\n",
    "\n",
    "    #         # Randomly pick a sample from validation dataset to encoder\n",
    "    #         data, *_ = self.valid_dataset[idx]\n",
    "    #         self.sampler.plot(data, orig_path)\n",
    "\n",
    "    #         # Add batch dimension and move it to device\n",
    "    #         data_batched = data.unsqueeze(1).to(self.device)\n",
    "    #         # Sample\n",
    "    #         self.sampler.sample(data_batched, self.hp.temperature, decoded_path)\n",
    "\n",
    "    #         if display:\n",
    "    #             Image.open(orig_path).show()\n",
    "    #             Image.open(decoded_path).show()\n",
    "    #         orig_paths.append(orig_path)\n",
    "    #         decoded_paths.append(decoded_path)\n",
    "    #     return sorted(orig_paths), sorted(decoded_paths)\n",
    "\n",
    "    def step(self, batch: Any, is_training=False):\n",
    "        self.model.train(is_training)\n",
    "\n",
    "        data = batch[0].to(self.device)\n",
    "        \n",
    "        inp = data\n",
    "        tar_inp = inp[:, :-1, ...]\n",
    "        tar_real = inp[:, 1:, ...]\n",
    "\n",
    "        enc_padding_mask, dec_padding_mask, dec_target_padding_mask, look_ahead_mask = create_masks(inp, tar_inp, device=self.device)\n",
    "\n",
    "        recon, _ = self.model(inp, tar_inp, enc_padding_mask, dec_padding_mask, dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        loss, loss_extras = self.loss(recon, tar_real)\n",
    "\n",
    "        # Only if we are in training state\n",
    "        if is_training:\n",
    "            # Set `grad` to zero\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # # Clip gradients\n",
    "            # nn.utils.clip_grad_norm_(self.model.parameters(), self.hp.grad_clip)\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer.step()\n",
    "        return data.shape[0], loss.item(), loss_extras\n",
    "\n",
    "    def validate_one_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_items, total_loss = 0, 0\n",
    "        with torch.no_grad(): \n",
    "            for batch in iter(self.valid_loader):\n",
    "                batch_items, loss, _ = self.step(batch, is_training=False)\n",
    "\n",
    "                total_loss += loss * batch_items\n",
    "                total_items += batch_items\n",
    "                \n",
    "        avg_loss = total_loss / total_items\n",
    "        self.log(dict(\n",
    "            val_avg_loss=avg_loss,\n",
    "            epoch=epoch))\n",
    "        return avg_loss\n",
    "\n",
    "    def train_one_epoch(self, epoch, parent_progressbar=None):\n",
    "        steps_per_epoch = len(self.train_loader)\n",
    "        for idx, batch in enumerate(progress_bar(iter(self.train_loader), parent=parent_progressbar)):\n",
    "            self.scheduler.step()\n",
    "            self.total_steps = idx + epoch * steps_per_epoch\n",
    "            _, loss, loss_extras = self.step(batch, is_training=True)\n",
    "            self.log(dict(\n",
    "                loss=loss,\n",
    "                epoch=epoch,\n",
    "                learning_rate=self.optimizer.param_groups[0]['lr'],\n",
    "                **loss_extras\n",
    "            ))\n",
    "        \n",
    "    def train(self):\n",
    "        mb = master_bar(range(self.hp.epochs))\n",
    "        for epoch in mb:\n",
    "            self.train_one_epoch(epoch=epoch, parent_progressbar=mb)\n",
    "            val_avg_loss = self.validate_one_epoch(epoch)\n",
    "            update_best_val = False\n",
    "            if val_avg_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_avg_loss\n",
    "                update_best_val = True\n",
    "                self.save()\n",
    "                # self.sample()\n",
    "            mb.write(f\"Finished epoch {epoch}. Validation Loss: {val_avg_loss}{' (new best)' if update_best_val else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
