{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sketch_transformer.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 enc_hidden_size: int, # d_model\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 max_seq_len: int,\n",
    "                 dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(enc_hidden_size, num_heads, dropout=dropout_prob, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(enc_hidden_size, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, enc_hidden_size),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm([enc_hidden_size]) # -1 instead of batch size?\n",
    "        self.ln2 = nn.LayerNorm([enc_hidden_size])\n",
    "        # self.ln1 = nn.LayerNorm([hp.max_seq_length+2, enc_hidden_size]) # -1 instead of batch size?\n",
    "        # self.ln2 = nn.LayerNorm([hp.max_seq_length+2, enc_hidden_size])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        \"\"\"\n",
    "        x will have shape `[seq_len, batch_size, 5]`\n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        out1 = self.ln1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.ln2(out1 + ffn_output)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 max_seq_len: int,\n",
    "                 dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.mha1 = nn.MultiheadAttention(d_model, num_heads, dropout=dropout_prob, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(d_model, num_heads, dropout=dropout_prob, batch_first=True)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm([d_model]) # -1 instead of batch size?\n",
    "        self.ln2 = nn.LayerNorm([d_model])\n",
    "        self.ln3 = nn.LayerNorm([d_model])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, enc_output: torch.Tensor, padding_mask, dec_target_padding_mask, look_ahead_mask):\n",
    "        \"\"\"\n",
    "        x will have shape `[batch_size, target_seq_len, d_model]`\n",
    "        enc_output will have shape `[batch_size, input_seq_len, d_model]`\n",
    "        \"\"\"\n",
    "\n",
    "        # print(f\"\\t\\t\\tDecoderLayer.forward: (MHA1 K,Q,V)     x={x.shape} \")\n",
    "        # print(f\"\\t\\t\\tDecoderLayer.forward: (MHA1 mask)      dec_target_padding_mask={dec_target_padding_mask.shape}\")\n",
    "        # print(f\"\\t\\t\\tDecoderLayer.forward: (MHA1 lookahead) look_ahead_mask={look_ahead_mask.shape}\")\n",
    "\n",
    "        attn1, attn1_weights = self.mha1(x, x, x, key_padding_mask=dec_target_padding_mask, attn_mask=look_ahead_mask)\n",
    "        out1 = self.ln1(x + attn1)\n",
    "\n",
    "        # print(f\"\\t\\t\\tDecoderLayer.forward: (MHA2 K,Q)  enc_output={enc_output.shape}\")\n",
    "        # print(f\"\\t\\t\\tDecoderLayer.forward: (MHA2 V)    out1={out1.shape}\")\n",
    "        # print(f\"\\t\\t\\tDecoderLayer.forward: (MAH2 mask) padding_mask={padding_mask.shape}\")\n",
    "\n",
    "\n",
    "        x2 = enc_output[:, :out1.shape[1], ...]\n",
    "        # print(f\"\\t\\t\\tDecoderLayer.forward: (MHA2 K,Q)  x2={x2.shape}   (CHOPPED)\")\n",
    "        \n",
    "        attn2, attn2_weights = self.mha2(x2, x2, out1, dec_target_padding_mask) #padding_mask)\n",
    "        out2 = self.ln2(out1 + attn2)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        out3 = self.ln3(ffn_output)\n",
    "        \n",
    "        return out3, attn1_weights, attn2_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 252, batch_size: int =100):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        # pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe = torch.zeros(batch_size, max_len, d_model)\n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        # chop stored positional encoding down:\n",
    "        # - in dimension 0, because during validation there may be a final batch with size < batch_size\n",
    "        # - in dimension 1, because during decoding/etc there may be smaller sequences passed in\n",
    "        x_pe = self.pe[:x.size(0), :x.size(1)]\n",
    "        x = x + x_pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute a single embedding based on a whole sequence of embedding outputs from\n",
    "    multi-head attention layers, as described in End-to-End Memory Networks:\n",
    "    https://arxiv.org/abs/1503.08895\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_lowerdim):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Linear(d_model, d_lowerdim)\n",
    "        self.W = nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(d_model))\n",
    "        self.V = nn.Parameter(torch.rand(d_model, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        u_i = tanh(xW + b)\n",
    "        a_i = softmax(u_i * V)\n",
    "        o = sum(a * x)\n",
    "\n",
    "        :param x: input tensor (batch_size, seq_len, d_model)\n",
    "        :return:  output tensor (batch_size, d_lowerdim)\n",
    "        \"\"\"\n",
    "        u_i = F.tanh((x @ self.W) + self.b) # (batch_size, seq_len, d_model)\n",
    "        a_i = F.softmax(u_i @ self.V, dim=1) # (batch_size, seq_len, 1)\n",
    "        o = torch.sum(x * a_i, dim=1) # (batch_size, seq_len, d_model)\n",
    "        o = self.embedding_layer(o)\n",
    "        return o, a_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DenseExpander(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.project_layer = nn.Linear(in_dim, out_dim)\n",
    "        self.expand_layer = nn.Linear(1, seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.project_layer(x))\n",
    "        x = x.unsqueeze(2) # (batch_size, out_dim, 1)\n",
    "        x = self.expand_layer(x) # (batch_size, out_dim, seq_len)\n",
    "        x = x.transpose(1, 2) # (batch_size, seq_len, out_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
