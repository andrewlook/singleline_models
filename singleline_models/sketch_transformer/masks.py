# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/sketch_transformer/01_masks.ipynb.

# %% auto 0
__all__ = ['create_padding_mask', 'create_lookahead_mask', 'create_masks', 'make_dummy_input']

# %% ../../nbs/sketch_transformer/01_masks.ipynb 4
import torch

# %% ../../nbs/sketch_transformer/01_masks.ipynb 6
def create_padding_mask(seq):
    """
    In seq, the 5th entry in the last dimension is the padding column, which will
    be 1 if the row is padding.
    
    Convert to a boolean tensor, indicating 'True' for entries that are padding and should be ignored.

    :param seq: (batch_size, seq_len, 5)
    :return: (batch_size, seq_len)
    """
    return seq[..., -1].bool()


def create_lookahead_mask(seq_len):
    """
    Create an attention mask, with rows representing target position and columns representing source position.

    For row=i, column=j, mask[i][j] is 'True' if the decoder must ignore position j when processing position i.

    An upper diagonal matrix (without the diagonal) will have 'True' for any j > i.
    
    :param seq_len: sequence length
    :return: (seq_len, seq_len)
    """
    return torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()


def create_masks(input_seq, target_seq, device='cuda'):
    enc_padding_mask = create_padding_mask(input_seq)

    # Used in the 2nd attention block in the decoder.
    # This padding mask is used to mask the encoder outputs.
    dec_padding_mask = create_padding_mask(input_seq)

    # Used in the 1st attention block in the decoder.
    # It is used to pad and mask future tokens in the input received by
    # the decoder.
    look_ahead_mask = create_lookahead_mask(target_seq.shape[1])
    dec_target_padding_mask = create_padding_mask(target_seq)

    # NOTE: torch nn.MHA takes separate padding & attn masks w/ different shapes,
    #       so use that instead of combining here
    return enc_padding_mask.to(device), dec_padding_mask.to(device), dec_target_padding_mask.to(device), look_ahead_mask.to(device)


def make_dummy_input(total_seq_len, nattn, batch_size, expected_len=None):
  nignore = total_seq_len - nattn
  return torch.cat([
      torch.ones(batch_size, nattn, 5) * torch.tensor([0., 0., 0., 0., 0.]),
      torch.ones(batch_size, nignore, 5) * torch.tensor([0., 0., 0., 0., 1.])
  ], dim=1)
