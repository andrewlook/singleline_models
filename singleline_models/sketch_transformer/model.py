# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/sketch_transformer/02_model.ipynb.

# %% auto 0
__all__ = ['Encoder', 'Decoder', 'ReconstructionLoss', 'Model']

# %% ../../nbs/sketch_transformer/02_model.ipynb 4
import math
from typing import Optional, Tuple

import torch
import torch.nn as nn
from torch.nn import functional as F

from .layers import (DecoderLayer, DenseExpander, EncoderLayer,
                     PositionalEncoding, SelfAttn)

# %% ../../nbs/sketch_transformer/02_model.ipynb 5
class Encoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, max_seq_len, dropout_rate):
        super().__init__()
        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = nn.Linear(5, d_model)
        self.pos_encoding = PositionalEncoding(d_model, dropout_rate, max_len=max_seq_len)

        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, max_seq_len=max_seq_len, dropout_prob=dropout_rate) for _ in range(num_layers)])

    def forward(self, x, mask):
        seq_len = x.shape[1]
        x = self.embedding(x) # (batch, seq, d_model)
        x *= torch.sqrt(torch.tensor(self.d_model))
        # add positional embedding, and apply dropout
        x = self.pos_encoding(x)

        for i, layer in enumerate(self.enc_layers):
            x = layer(x, mask)
        return x # (batch, seq, d_model)

# %% ../../nbs/sketch_transformer/02_model.ipynb 6
class Decoder(nn.Module):

    def __init__(self, num_layers, d_model, num_heads, d_ff,
                    max_seq_len=1000,
                    dropout_rate=0.1):
        super().__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = nn.Linear(5, d_model)
        self.pos_encoding = PositionalEncoding(d_model, dropout_rate, max_len=max_seq_len)

        self.dec_layers = nn.ModuleList([DecoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff, max_seq_len=max_seq_len, dropout_prob=dropout_rate)
                            for _ in range(num_layers)])

    def forward(self, x, enc_output, padding_mask, dec_target_padding_mask, look_ahead_mask):
        seq_len = x.shape[1]
        x = self.embedding(x) # (batch, seq, d_model)
        x *= torch.sqrt(torch.tensor(self.d_model))
        # add positional embedding, and apply dropout
        x = self.pos_encoding(x)

        # print(f"\t\tDecoder.forward: x={x.shape}")
        # print(f"\t\tDecoder.forward: enc_output={enc_output.shape}")

        # print(f"\t\tDecoder.forward: padding_mask={padding_mask.shape}")
        # print(f"\t\tDecoder.forward: dec_target_padding_mask={dec_target_padding_mask.shape}")
        # print(f"\t\tDecoder.forward: look_ahead_mask={look_ahead_mask.shape}")

        attention_weights = {}
        for i, layer in enumerate(self.dec_layers):
            x, block1, block2 = layer(x, enc_output, padding_mask, dec_target_padding_mask, look_ahead_mask)

        attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
        attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2

        # x.shape == (batch_size, target_seq_len, d_model)
        return x, attention_weights

# %% ../../nbs/sketch_transformer/02_model.ipynb 7
class ReconstructionLoss(nn.Module):
   
    def forward(self, pred, real):
        pred_locations = pred[:, :, :2]
        real_locations = real[:, :, :2]
        location_loss = F.mse_loss(pred_locations, real_locations)

        pred_metadata = pred[:, :, 2:] # un-normalized logits
        real_metadata = real[:, :, 2:] # true labels in probability spaces (add up to 1).

        # for K-dimensional inputs, torch cross_entropy() requires:
        # - batch as first dimension,
        # - class as second,
        # - ... other dimensions
        pred_metadata = pred_metadata.transpose(2, 1) # [batch_size, n_classes, n_seq]
        real_idx = torch.argmax(real_metadata.transpose(2, 1), dim=1) # [batch_size, n_seq]

        metadata_loss = F.cross_entropy(pred_metadata, real_idx)

        loss = location_loss + metadata_loss

        # import ipdb; ipdb.set_trace()
        
        return loss

# %% ../../nbs/sketch_transformer/02_model.ipynb 8
class Model(nn.Module):
    def __init__(self, hp):
        super().__init__()
        self.hp = hp
        self.encoder = Encoder(
            num_layers=hp.n_layer,
            d_model=hp.d_model,
            num_heads=hp.n_head,
            d_ff=hp.d_ff,
            max_seq_len=hp.max_seq_length+2,
            dropout_rate=hp.dropout_rate)
        self.bottleneck_layer = SelfAttn(d_model=hp.d_model, d_lowerdim=hp.d_lowerdim)
        self.expand_layer = DenseExpander(in_dim=hp.d_lowerdim, out_dim=hp.d_model, seq_len=hp.max_seq_length+2)
        self.decoder = Decoder(num_layers=hp.n_layer,
            d_model=hp.d_model,
            num_heads=hp.n_head,
            d_ff=hp.d_ff,
            max_seq_len=hp.max_seq_length+2,
            dropout_rate=hp.dropout_rate)
        self.output_layer = nn.Linear(hp.d_model, 5)
    
    def encode(self, x, mask):
        enc_output = self.encoder(x, mask)
        lowerdim_output, _ = self.bottleneck_layer(enc_output)
        return lowerdim_output, enc_output
    
    def decode(self, embedding, target, dec_padding_mask, dec_target_padding_mask, look_ahead_mask):
        """Generate logits"""
        padding_mask = torch.zeros_like(dec_padding_mask) if self.hp.blind_decoder_mask else dec_padding_mask
        pre_decoder = self.expand_layer(embedding)

        # print(f"\tModel.decode: embedding={embedding.shape}")
        # print(f"\tModel.decode: pre_decoder={pre_decoder.shape}")

        # print(f"\tModel.decode: target={target.shape}")
        
        # print(f"\tModel.decode: dec_padding_mask={padding_mask.shape}")
        # print(f"\tModel.decode: dec_target_padding_mask={dec_target_padding_mask.shape}")
        # print(f"\tModel.decode: look_ahead_mask={look_ahead_mask.shape}")
        
        dec_output, attention_weights = self.decoder(
            x=target,
            enc_output=pre_decoder,
            padding_mask=padding_mask,
            dec_target_padding_mask=dec_target_padding_mask,
            look_ahead_mask=look_ahead_mask)
        final_output = self.output_layer(dec_output)
        return final_output, attention_weights

    def forward(self, input_seq, target_seq, enc_padding_mask, dec_padding_mask, dec_target_padding_mask, look_ahead_mask):
        lowerdim_output, enc_output = self.encode(input_seq, enc_padding_mask)
        # print(f"\tModel.forward: lowerdim_output.shape={lowerdim_output.shape}")
        # print(f"\tModel.forward: enc_output.shape={enc_output.shape}")
        
        final_output, attention_weights = self.decode(
            embedding=lowerdim_output,
            target=target_seq,
            dec_padding_mask=dec_padding_mask,
            dec_target_padding_mask=dec_target_padding_mask,
            look_ahead_mask=look_ahead_mask)
        return final_output, lowerdim_output #, enc_output, attention_weights

    # def encode_from_seq(self, inp_seq):
    #     """same as encode but compute mask inside. Useful for test"""
    #     dtype = tf.float32 if self.dataset.hps['use_continuous_data'] else tf.int64
    ##
    ## i think that last element of the tensor below is to account for train-time
    ## shifting off-by for for input vs target...
    ##
    #     encoder_input = tf.cast(np.array(inp_seq) + np.zeros((1, 1)), dtype)  # why?
    #     enc_padding_mask = builders.utils.create_padding_mask(encoder_input)
    #     res = self.encode(encoder_input, enc_padding_mask, training=False)
    #     return res
  
      
